{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4gKAY1c6EPZh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_data(url):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        text_data = response.text\n",
        "        return text_data\n",
        "    else:\n",
        "        raise Exception(f\"Failed to retrieve the content. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "zb5RDfecICth"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_data(text_data, seq_length):\n",
        "    chars = sorted(list(set(text_data)))\n",
        "    char_to_index = {char: i for i, char in enumerate(chars)}\n",
        "    index_to_char = {i: char for i, char in enumerate(chars)}\n",
        "\n",
        "    sequences = []\n",
        "    next_chars = []\n",
        "\n",
        "    for i in range(len(text_data) - seq_length):\n",
        "        seq = text_data[i:i + seq_length]\n",
        "        next_char = text_data[i + seq_length]\n",
        "        sequences.append(seq)\n",
        "        next_chars.append(next_char)\n",
        "\n",
        "    X = np.zeros((len(sequences), seq_length, len(chars)), dtype=np.bool)\n",
        "    y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
        "\n",
        "    for i, seq in enumerate(sequences):\n",
        "        for t, char in enumerate(seq):\n",
        "            X[i, t, char_to_index[char]] = 1\n",
        "        y[i, char_to_index[next_chars[i]]] = 1\n",
        "\n",
        "    return X, y, char_to_index, index_to_char"
      ],
      "metadata": {
        "id": "jWaLRhQZGO6C"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(seq_length, num_chars):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=(seq_length, num_chars)))\n",
        "    model.add(Dense(num_chars, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model"
      ],
      "metadata": {
        "id": "jS3P3JOlGPKA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, epochs=20, batch_size=64, validation_split=0.1):\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n"
      ],
      "metadata": {
        "id": "yJz8ODCBGPQv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_text, char_to_index, index_to_char, seq_length, temperature=1.0, num_chars=100):\n",
        "    generated_text = start_text\n",
        "\n",
        "    for _ in range(num_chars):\n",
        "        x_pred = np.zeros((1, seq_length, len(char_to_index)))\n",
        "        for t, char in enumerate(start_text):\n",
        "            x_pred[0, t, char_to_index[char]] = 1.0\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds, temperature)\n",
        "        next_char = index_to_char[next_index]\n",
        "\n",
        "        generated_text += next_char\n",
        "        start_text = start_text[1:] + next_char\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "bUQ86avIGPXQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/abdoelsayed2016/HKR_Dataset/master/LICENSE.CC-BY-NC-ND-4.0\"\n",
        "text_data = load_text_data(url)"
      ],
      "metadata": {
        "id": "5Qezw6I6GPjS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 50"
      ],
      "metadata": {
        "id": "TqAiHywqGPmA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, char_to_index, index_to_char = preprocess_text_data(text_data, seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FkV71adGPqc",
        "outputId": "e91a6203-2e18-44f8-d396-a304f11bcbe5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-8439783d744f>:15: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X = np.zeros((len(sequences), seq_length, len(chars)), dtype=np.bool)\n",
            "<ipython-input-26-8439783d744f>:16: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(seq_length, num_chars):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=(seq_length, num_chars)))\n",
        "    model.add(Dense(num_chars, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model"
      ],
      "metadata": {
        "id": "ae1L59HFGPr8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, epochs=20, batch_size=64, validation_split=0.1):\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n"
      ],
      "metadata": {
        "id": "_y0Zbva8GPtc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_text, char_to_index, index_to_char, seq_length, temperature=1.0, num_chars=100):\n",
        "    generated_text = start_text\n",
        "\n",
        "    for _ in range(num_chars):\n",
        "        x_pred = np.zeros((1, seq_length, len(char_to_index)))\n",
        "        for t, char in enumerate(start_text):\n",
        "            x_pred[0, t, char_to_index[char]] = 1.0\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds, temperature)\n",
        "        next_char = index_to_char[next_index]\n",
        "\n",
        "        generated_text += next_char\n",
        "        start_text = start_text[1:] + next_char\n",
        "\n",
        "    return generated_text\n"
      ],
      "metadata": {
        "id": "D_E-XTzGGPv6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "kKClGmQYGPy8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = X.shape[1]\n",
        "num_chars = X.shape[2]\n",
        "model = build_model(seq_length, num_chars)\n",
        "train_model(model, X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC4K8jKdGP2d",
        "outputId": "3aecf38d-aac1-4fb8-f20a-0bd924a3cd17"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "211/211 [==============================] - 4s 9ms/step - loss: 3.0165 - val_loss: 2.8834\n",
            "Epoch 2/20\n",
            "211/211 [==============================] - 2s 7ms/step - loss: 2.7172 - val_loss: 2.6253\n",
            "Epoch 3/20\n",
            "211/211 [==============================] - 1s 5ms/step - loss: 2.4115 - val_loss: 2.3086\n",
            "Epoch 4/20\n",
            "211/211 [==============================] - 1s 6ms/step - loss: 2.1664 - val_loss: 2.1443\n",
            "Epoch 5/20\n",
            "211/211 [==============================] - 1s 6ms/step - loss: 2.0107 - val_loss: 2.0130\n",
            "Epoch 6/20\n",
            "211/211 [==============================] - 1s 5ms/step - loss: 1.8982 - val_loss: 1.9466\n",
            "Epoch 7/20\n",
            "211/211 [==============================] - 1s 5ms/step - loss: 1.8067 - val_loss: 1.8808\n",
            "Epoch 8/20\n",
            "211/211 [==============================] - 1s 5ms/step - loss: 1.7342 - val_loss: 1.8150\n",
            "Epoch 9/20\n",
            "211/211 [==============================] - 1s 5ms/step - loss: 1.6670 - val_loss: 1.7625\n",
            "Epoch 10/20\n",
            "211/211 [==============================] - 1s 5ms/step - loss: 1.6076 - val_loss: 1.7262\n",
            "Epoch 11/20\n",
            "211/211 [==============================] - 1s 6ms/step - loss: 1.5542 - val_loss: 1.6889\n",
            "Epoch 12/20\n",
            "211/211 [==============================] - 1s 7ms/step - loss: 1.5041 - val_loss: 1.6321\n",
            "Epoch 13/20\n",
            "211/211 [==============================] - 1s 6ms/step - loss: 1.4576 - val_loss: 1.6397\n",
            "Epoch 14/20\n",
            "211/211 [==============================] - 1s 6ms/step - loss: 1.4088 - val_loss: 1.5978\n",
            "Epoch 15/20\n",
            "211/211 [==============================] - 1s 5ms/step - loss: 1.3669 - val_loss: 1.5924\n",
            "Epoch 16/20\n",
            "211/211 [==============================] - 1s 5ms/step - loss: 1.3274 - val_loss: 1.5532\n",
            "Epoch 17/20\n",
            "211/211 [==============================] - 1s 5ms/step - loss: 1.2926 - val_loss: 1.5314\n",
            "Epoch 18/20\n",
            "211/211 [==============================] - 1s 5ms/step - loss: 1.2576 - val_loss: 1.5150\n",
            "Epoch 19/20\n",
            "211/211 [==============================] - 1s 5ms/step - loss: 1.2201 - val_loss: 1.5263\n",
            "Epoch 20/20\n",
            "211/211 [==============================] - 1s 6ms/step - loss: 1.1886 - val_loss: 1.4951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_text = \"Lorem ipsum \"\n",
        "generated_text = generate_text(model, start_text, char_to_index, index_to_char, seq_length, temperature=0.5, num_chars=200)\n"
      ],
      "metadata": {
        "id": "X1btvNiuIrnp"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0NNeUb8Irq1",
        "outputId": "908a932a-bf1a-4d13-af04-51268eeb13db"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lorem ipsum ardteceeete t teateui rteet uaaae ttiieeeeteo iorree tedeeecgdeiadorecterettaiaee ttgctt  teiote ieeeoeeyrrg tgeat tiaotueeetrige ttetw ieertriiatasttigt eiiteoeurc erteereteomtettteeetretd\n",
            "tgpig,ers \n"
          ]
        }
      ]
    }
  ]
}